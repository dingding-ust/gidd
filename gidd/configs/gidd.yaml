defaults:
  - logging: default
  - data: owt
  - model: small
  - optimizer: adam
  - training
  - _self_

model:
  type: diffusion
  diffusion_process: gidd
  p_uniform: 0.0
  t_eps: 1e-4

training:
  resume: null
  seed: 1
  train_batch_size: 32
  eval_batch_size: 64
  num_train_steps: 500_000
  lr_schedule: cosine
  warmup_steps: 10000
  low_discrepancy_sampling: True
  dtype: bf16
  compile_model: True

loss:
  loss_type: gidd
  loss_weighting: dynamic  # [dynamic, clip, none]
  min_loss_weight: 0.0
  max_loss_weight: 1.0
  loss_scale: 1.0
  reduction: tokenmean
